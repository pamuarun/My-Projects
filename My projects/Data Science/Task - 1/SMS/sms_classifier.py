# -*- coding: utf-8 -*-
"""SMS Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SyJpgn034t0WO3Qj0tobO4WXIH4Hj8ZO

# **Step 1- Reading and Understanding Data**

Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# import required libraries for dataframe and visualization
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import nltk
from nltk.corpus import stopwords
from collections import Counter

# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

nltk.download('stopwords')

# load data
df = pd.read_csv('/content/spam.csv',  encoding='latin-1')
df.head()

# Drop unnecessary columns from the DataFrame

columns_to_drop = ["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"]
df.drop(columns=columns_to_drop, inplace=True)
df.head()

df.shape

# data information
df.info()

# Finding the count of null values
df.isnull().sum()

"""No null values in the dataset"""

# RENAMING THE COLUMNS
df.columns = ['category', 'message']
df.head()

#lets make another column i.e the length of the sms
len_msg=[]
for i in df['message']:
    len_msg.append(len(i))

df['msg_length']=len_msg
df.head()

"""#**Step2 : Visualizing and Data Cleaning**

"""

plt.figure(figsize=(13,5))

custom_colors = ['green', 'orange']
ax =plt.subplot(1,3,1)
ax= df['category'].value_counts().plot.pie(explode=[0.1, 0.1],autopct='%1.2f%%', colors = custom_colors, shadow=True);
ax.set_title(label = "Distribution of msgs ", fontsize = 20, color= 'Red');

ax = plt.subplot(1,3,3)
ax = sns.countplot(x='category', data=df, width=0.1)
plt.title("Distribution of msgs", fontsize=20)
sns.countplot(data=df, x='category', palette = "Dark2")
plt.xlabel('Category')
plt.show()

sns.set_style('whitegrid')

f, ax = plt.subplots(1, 2, figsize = (20, 6))

sns.distplot(df[df['category']=='spam']['msg_length'], bins = 20, color = 'orange', ax = ax[0])
ax[0].set_xlabel("Spam text Length")

sns.distplot(df[df['category']=='ham']['msg_length'], bins = 20,color='green', ax = ax[1])
ax[1].set_xlabel("Ham text Length")



plt.show()

"""# **Step 3: Tokenization and Tokens visualization**"""

from wordcloud import WordCloud

wc = WordCloud(background_color='white', max_words=200 )
wc.generate(' '.join(text for text in df.loc[df['category'] == 'ham', 'message']))
plt.figure(figsize=(9,5), facecolor='k')
plt.title('Most repeated words in HAM messages', color= 'white', fontdict={'size': 22, 'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()

wc = WordCloud(background_color='orange', max_words=200)
wc.generate(' '.join(text for text in df.loc[df['category'] == 'spam', 'message']))
plt.figure(figsize=(9,5))
plt.title('Most repeated words in SPAM messages',
          fontdict={'size': 22,  'verticalalignment': 'bottom'})
plt.imshow(wc)
plt.axis("off")
plt.show()

"""# **Step 4 : Data Preparation & Target encoding**"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(df['category'])

df['category_encoded'] = le.transform(df['category'])
df.head()

# Separate the feature (message) and target (category) data

X = df["message"]
Y = df["category_encoded"]

"""Splitting the data into training data & Test data

"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)
print(X.shape, X_train.shape, X_test.shape)

# Convert Y_train and Y_test to integer type
Y_train = Y_train.astype("int")
Y_test = Y_test.astype("int")

print(X_train)

"""# **Step 5:Feature Extraction**

Model Training : Logistic Regression
"""

# Initialize TF-IDF Vectorizer
feature_extraction = TfidfVectorizer(min_df=1, stop_words="english", lowercase=True)

# Feature extraction for training and testing data
X_train_features = feature_extraction.fit_transform(X_train)
X_test_features = feature_extraction.transform(X_test)

# training the Logistic Regression model with training data

model = LogisticRegression()
model.fit(X_train_features, Y_train)

"""Accuracy Score"""

# accuracy on training data
X_train_prediction = model.predict(X_train_features)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)
print('Accuracy score of training data : ', training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test_features)

test_data_accuracy = accuracy_score(Y_test, X_test_prediction)
print('Accuracy score of test data : ', test_data_accuracy)

# Calculate and print Root Mean Square Error(RMSE)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(Y_test, X_test_prediction)
rmse = np.sqrt(mse)
print("RMSE value: {:.4f}".format(rmse))

#Make predictions on the training data
predict_train_data=model.predict(X_train_features)

#Model Evaluation
from sklearn.metrics import accuracy_score,confusion_matrix
accuracy_train_data=accuracy_score(Y_train,predict_train_data)
print("Accuracy on training data: ",accuracy_train_data)

# Make predictions on the testing data
predict_test_data=model.predict(X_test_features)

#Model Evaluation
accuracy_test_data=accuracy_score(Y_test,predict_test_data)
print("acuuracy on test data: ",accuracy_test_data)

"""# **Step 6 : Model testing**

**Test the model with few randon messages**
"""

new_msg=["Need to talk to you.. call me "]
new_data_features=feature_extraction.transform(new_msg)
prediction=model.predict(new_data_features)
print(prediction)


if(prediction[0]==1):
    print("SPAM Message")
else:
    print("HAM Message")

new_msg=["Free entry in 2 a wkly comp to win FA Cup fina"]
new_data_features=feature_extraction.transform(new_msg)
prediction=model.predict(new_data_features)
print(prediction)


if(prediction[0]==1):
    print("SPAM Message")
else:
    print("HAM Message")

"""## Confusion Matrix"""

conf_matrix=confusion_matrix(Y_test,predict_test_data)
plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix,annot=True,fmt="d",cmap="Greens",cbar=False)
plt.xlabel("Predicted value")
plt.ylabel("Actual value")
plt.title("Confusion Matrix")
plt.show()

