# -*- coding: utf-8 -*-
"""Titanic Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IoKkGdXwZQG6CdMndiM608At4DG6X87L
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression

train = pd.read_csv('/content/titanic.csv')

train.head()

sns.heatmap(train.isnull(),yticklabels=False)

sns.countplot(x='Survived', data=train)

sns.countplot(x='Survived', data=train, hue='Sex')

sns.countplot(x='Survived', data=train, hue='Pclass')

sns.distplot(train.Age.dropna(), kde=False, bins=40)

sns.distplot(train.Fare.dropna(), kde=False, bins=45)

sns.countplot(train.SibSp)

plt.figure(figsize=(11,8))
sns.boxplot(x='Pclass', y='Age', data=train)

def calc_age(col):
    Age = col[0]
    Pass_class = col[1]

    if pd.isnull(Age):

        if Pass_class == 1:
            return 37
        elif Pass_class == 2:
            return 29
        else:
            return 24
    else:
        return Age
train['Age'] = train[['Age','Pclass']].apply(calc_age, axis=1)

# Missing age values have been filled
sns.heatmap(train.isnull(),yticklabels=False)

train.drop('Cabin', axis=1, inplace=True)

train.dropna(inplace=True)
sns.heatmap(train.isnull(),yticklabels=False)

# Handling catagorical features
binarysex = pd.get_dummies(train['Sex'],drop_first=True)
embarked = pd.get_dummies(train['Embarked'], drop_first=True)
Passengerclass = pd.get_dummies(train['Pclass'],drop_first=True)
train.drop(['Ticket','Embarked','Name','Sex','PassengerId','Pclass'], axis=1, inplace=True)

# drop one column to avoid multicollinearity
train = pd.concat([binarysex,embarked,Passengerclass,train], axis = 1)

train.head()

# Extracting the training set and test set from the data
X = train.drop('Survived', axis = 1)
y = train['Survived']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

# Cleaning and preparing the new Xdata
NewX = pd.read_csv('/content/titanic.csv')

sns.heatmap(NewX.isnull(),yticklabels=False)

NewX['Age'] = NewX[['Age','Pclass']].apply(calc_age, axis=1)
NewX.drop('Cabin', axis=1, inplace=True)
NewX.dropna(inplace=True)

sns.heatmap(NewX.isnull(),yticklabels=False)

# Convert feature names to strings for both training and testing data
X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

# Fitting and applying the logistic regression model with increased max_iter
logmodel = LogisticRegression(max_iter=1000)
logmodel.fit(X_train, y_train)
y_pred = logmodel.predict(X_test)
accuracy = logmodel.score(X_test, y_test)

print("Accuracy:", accuracy)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)

# Assuming 'Survived' is the target variable to predict
# If it's not, adjust accordingly

# Convert feature names to strings for both training and new data
X.columns = X.columns.astype(str)
NewX.columns = NewX.columns.astype(str)

# Fitting the logistic regression model on the training data
logmodel.fit(X, y)

# Remove the 'Survived' column from NewX if it's present
if 'Survived' in NewX.columns:
    NewX.drop('Survived', axis=1, inplace=True)

# Drop unnecessary columns from NewX
NewX.drop([ 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)

# Convert feature names to strings for both training and new data
X.columns = X.columns.astype(str)
NewX.columns = NewX.columns.astype(str)

# Predicting the new data
y_pred2 = logmodel.predict(NewX)
print(y_pred2)

# Tuning the C value
# Fitting and applying the logistic regression model
import numpy as np
max_score=[]
best_c = []
inter = [0.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5,1.7,1.9,2.1,2.2,2.3,2.5,2.7,2.9,3.3,3.5,3.7,3.9,4.1]
for num in inter:
    logmodel = LogisticRegression(C = num)
    logmodel.fit(X_train,y_train)
    score = logmodel.score(X_test,y_test)
    max_score.append(score)
    best_c.append(num)

print(max_score)
print(best_c)

plt.scatter(best_c,max_score)

# using the best C we found in our findings
logmodel = LogisticRegression(C = 0.1)
logmodel.fit(X,y)
y_pred2 = logmodel.predict(NewX)
print(y_pred2)

# Fitting the logistic regression model on the training data with the chosen C
logmodel = LogisticRegression(C=0.1)
logmodel.fit(X, y)

# Remove the 'Survived' column from NewX if it's present
if 'Survived' in NewX.columns:
    NewX.drop('Survived', axis=1, inplace=True)

# Predicting the new data with probabilities
y_probabilities = logmodel.predict_proba(NewX)

# Setting a threshold of 0.5 to convert probabilities to binary predictions
threshold = 0.5
y_pred_binary = (y_probabilities[:, 1] >= threshold).astype(int)

# Counting the number of 0s and 1s
num_survived = sum(y_pred_binary)
num_not_survived = len(y_pred_binary) - num_survived

print("Number of survived:", num_survived)
print("Number of not survived:", num_not_survived)

